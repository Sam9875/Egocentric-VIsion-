# ğŸ© Natural Language Queries in Egocentric Videos (Ego4D NLQ)

This project explores **natural language query (NLQ) grounding in egocentric videos** using the **Ego4D dataset** and the **VSLNet architecture**. It supports both baseline models and extended approaches leveraging **LLMs/VLMs** for enhanced performance.

---

## ğŸ“ Project Structure

| File | Description |
|------|-------------|
| `main.py` | Core pipeline for training and evaluation |
| `VSLNet.py` | Definitions for VSLBase and VSLNet models |
| `data_loader.py` | Data loading and preprocessing routines |
| `data_gen.py` | Converts datasets and creates vocab/embeddings |
| `layers.py` | Core model components (encoders, attention, prediction layers) |

âœ… **Note:** This implementation uses **BERT embeddings** for improved contextual understanding (replacing GloVe).

---

## ğŸ¯ Project Objective

Given a **natural language query** and a **video from the Ego4D dataset**, the goal is to **localize the segment** of the video that best answers the query.

### Example Queries:
- ğŸ—£ï¸ *â€œWhere was the phone last seen?â€*
- ğŸ—£ï¸ *â€œHow many frying pans can I see on the shelf?â€*

The project follows the **Ego4D NLQ benchmark** and implements:
- Baselines using **VSLBase/VSLNet**
- Feature extractors: **Omnivore** and **EgoVLP**
- **Extension**: Generating **textual answers** from localized segments using **Video-LLaVA**

---

## âš™ï¸ Installation

Install the required dependencies:

```bash
pip install torch transformers submitit nltk tqdm
```

Add any additional packages as needed (especially if using Jupyter notebooks).

---

## ğŸ“¦ Dataset Setup

1. **Request access** and agree to the [Ego4D license](https://ego4d-data.org).
2. **Download** annotations and video features:

```bash
ego4d --download-annotations nlq
ego4d --download-features omnivore_video_swinl_fp16
```

3. **Organize your files** as follows:

```
data/
â”œâ”€â”€ features/
â”‚   â””â”€â”€ <task>/<feature_version>/
â””â”€â”€ dataset/
    â””â”€â”€ <task>/
        â”œâ”€â”€ train.json
        â”œâ”€â”€ val.json
        â””â”€â”€ test.json
```

ğŸ’¡ *You can skip downloading if you already have the preprocessed features.*

---

## ğŸš€ Running the Model

### ğŸ‹ï¸ Training

```bash
python main.py     --mode train     --task ego4d     --fv omnivore     --max_pos_len 128     --predictor rnn     --epochs 10     --batch_size 32
```

Adjust `--batch_size` based on your hardware.

### ğŸ§ª Evaluation

```bash
python main.py     --mode test     --task ego4d     --fv omnivore     --predictor rnn
```

ğŸ“Œ *Default text embedding uses BERT. Use `--predictor glove` if you'd prefer GloVe.*

Use **TensorBoard** to monitor loss curves:

```bash
tensorboard --logdir=runs
```

---

## ğŸ” Extension: From Segment to Answer

After retrieving the relevant **video clip via NLQ**, we pass the **query + segment** into **Video-LLaVA** to generate a **natural language answer**.

This bridges **temporal localization** and **video question answering (VideoQA)** in a unified pipeline.

---

## ğŸ“Š Results

Performance is evaluated via:

- ğŸ§­ **Recall@K** at various tIoU thresholds (for localization)
- ğŸ“ **ROUGE / BLEU** scores (for textual answer generation)

---

## ğŸ““ Jupyter Notebooks

Notebooks are included for:
- Exploratory data analysis
- Feature visualization
- Query-based qualitative results
- Training and evaluation walkthroughs

---

## ğŸ“š References

- Ego4D Dataset â€” *Grauman et al., 2022*
- VSLNet â€” *Zhang et al., 2020*
- Omnivore â€” *Girdhar et al., 2022*
- EgoVLP â€” *Lin et al., 2022*
- Video-LLaVA â€” *Lin et al., 2023*

> âš ï¸ Some implementation insights were inspired by community codebases, research papers, and large language models.

---

## ğŸ™ Acknowledgements

Developed as part of the **Egocentric Video Understanding** course under the guidance of:

- **Simone Alberto Peirone**
- **Gaetano Salvatore Falco**
